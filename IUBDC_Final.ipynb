{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "z9NnRg7j6-gV",
        "outputId": "8f6c4fad-37c1-483e-8dd0-d94a46df7adb"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'ECG (EO, AC1, AC2).xlsx'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-d5206054914a>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m# Read the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mdata_ECG\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_ECG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msheet_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"EO\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0mdata_ECG\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Segment\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"EO\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36mread_excel\u001b[0;34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    476\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m         \u001b[0mshould_close\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 478\u001b[0;31m         \u001b[0mio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    479\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    480\u001b[0m         raise ValueError(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path_or_buffer, engine, storage_options)\u001b[0m\n\u001b[1;32m   1494\u001b[0m                 \u001b[0mext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xls\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1495\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1496\u001b[0;31m                 ext = inspect_excel_format(\n\u001b[0m\u001b[1;32m   1497\u001b[0m                     \u001b[0mcontent_or_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1498\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36minspect_excel_format\u001b[0;34m(content_or_path, storage_options)\u001b[0m\n\u001b[1;32m   1369\u001b[0m         \u001b[0mcontent_or_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent_or_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1371\u001b[0;31m     with get_handle(\n\u001b[0m\u001b[1;32m   1372\u001b[0m         \u001b[0mcontent_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1373\u001b[0m     ) as handle:\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    866\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m             \u001b[0;31m# Binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 868\u001b[0;31m             \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    869\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'ECG (EO, AC1, AC2).xlsx'"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow.keras as keras\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Flatten, Dense, Activation, Dropout, BatchNormalization\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "\n",
        "#make a main module\n",
        "if __name__ == \"__main__\":\n",
        "        # File paths\n",
        "    path_ECG = \"ECG (EO, AC1, AC2).xlsx\"\n",
        "    path_EEG = \"EEG (EO, AC1, AC2).xlsx\"\n",
        "    path_ratio = \"Ratio of Alpha _ Beta Power.xlsx\"\n",
        "\n",
        "    # Number of people (assuming this is useful for later steps)\n",
        "    num_of_people = 40\n",
        "\n",
        "    # Read the data\n",
        "\n",
        "    data_ECG = pd.read_excel(path_ECG, sheet_name=\"EO\")\n",
        "    data_ECG[\"Segment\"] = \"EO\"\n",
        "\n",
        "    for stress in [\"AC1\", \"AC2\"]:\n",
        "        data = pd.read_excel(path_ECG, sheet_name=stress)\n",
        "        data[\"Segment\"] = stress\n",
        "        data_ECG = pd.concat([data_ECG, data], ignore_index=True)\n",
        "\n",
        "    data_EEG = pd.read_excel(path_EEG, sheet_name=\"Normalize\")\n",
        "    data_EEG = data_EEG.ffill()\n",
        "\n",
        "    data = pd.merge(data_ECG, data_EEG, on=[\"Segment\", \"Subject (No.)\"])\n",
        "    for waves in [\"Alpha\", \"Beta1\", \"Beta2\"]:\n",
        "        data_ratio = pd.read_excel(path_ratio, sheet_name=waves)\n",
        "        data_ratio[\"Wave\"] = waves\n",
        "\n",
        "        data_ratio = data_ratio.T\n",
        "        data_ratio = data_ratio.ffill()\n",
        "        data_ratio = data_ratio.T\n",
        "\n",
        "        stresses = data_ratio.iloc[0]\n",
        "        new_header = data_ratio.iloc[1]\n",
        "\n",
        "        data_ratio.columns = list(data_ratio.columns[:2]) + list(new_header[2:])\n",
        "        data_ratio = data_ratio.drop([1])\n",
        "        head = data_ratio.iloc[:, 0:2]\n",
        "\n",
        "\n",
        "        data_ratio_ab = data_ratio.iloc[:, :6]\n",
        "        data_ratio_ab[\"Segment\"] = \"EO\"\n",
        "\n",
        "        for i, stress in enumerate([\"AC1\", \"AC2\"]):\n",
        "            temp = data_ratio.iloc[:, 6 + 4 * i:10 + 4 * i].copy()\n",
        "            temp[\"Segment\"] = stress\n",
        "            temp = pd.concat([head, temp], axis=1)\n",
        "            temp = temp.drop([0])\n",
        "            data_ratio_ab = pd.concat([data_ratio_ab, temp], ignore_index=True)\n",
        "\n",
        "        data_ratio_ab2 = data_ratio.iloc[:, 6 + 4 * (i+1):10 + 4 * (i+1)].copy()\n",
        "        data_ratio_ab2[\"Segment\"] = \"EO\"\n",
        "        data_ratio_ab2 = pd.concat([head, data_ratio_ab2], axis=1)\n",
        "        data_ratio_ab2 = data_ratio_ab2.drop([0])\n",
        "        i+=1\n",
        "\n",
        "        for j, stress in enumerate([\"AC1\", \"AC2\"]):\n",
        "            temp = data_ratio.iloc[:, 6 + 4 * (i+1):10 + 4 * (i+1)].copy()\n",
        "            temp[\"Segment\"] = stress\n",
        "            temp = pd.concat([head, temp], axis=1)\n",
        "            temp = temp.drop([0])\n",
        "            data_ratio_ab2 = pd.concat([data_ratio_ab2, temp], ignore_index=True)\n",
        "\n",
        "\n",
        "        data_ratio = pd.merge(data_ratio_ab2, data_ratio_ab, on=[\"Segment\", \"Subject (No.)\"])\n",
        "        data_ratio = data_ratio.rename(columns={\"Gender_x\": \"Gender\"})\n",
        "        data_ratio = data_ratio.drop(columns=[\"Gender_y\"])\n",
        "\n",
        "\n",
        "        data = pd.merge(data, data_ratio, on=[\"Segment\", \"Subject (No.)\"])\n",
        "        try:\n",
        "            data = data.rename(columns={\"Gender_x\": \"Gender\"})\n",
        "            data = data.drop(columns=[\"Gender_y\"])\n",
        "        except:\n",
        "            pass\n",
        "    data['Gender'] = data['Gender'].replace({'Male': 0, 'Female': 1})\n",
        "    data['Segment'] = data['Segment'].replace({'EO': 0, 'AC1': 1, 'AC2': 2})\n",
        "\n",
        "    print(data.shape)\n",
        "\n",
        "    # Shuffle the data\n",
        "    data = data.sample(frac=1)\n",
        "    print(data)\n",
        "    # Split the data\n",
        "    split_index = int(len(data) * 0.7)\n",
        "    train_data = data[:split_index]\n",
        "    test_data = data[split_index:]\n",
        "\n",
        "    # Prepare the input features and target labels\n",
        "    x_train = train_data.drop(columns=['Segment', 'Subject (No.)']).astype('float32')\n",
        "    y_train = train_data['Segment']\n",
        "    x_test = test_data.drop(columns=['Segment', 'Subject (No.)']).astype('float32')\n",
        "    y_test = test_data['Segment']\n",
        "\n",
        "    # Convert target labels to one-hot encoding\n",
        "    num_classes = len(y_train.unique())\n",
        "    y_train = to_categorical(y_train, num_classes)\n",
        "    y_test = to_categorical(y_test, num_classes)\n",
        "\n",
        "    \"\"\"\"\"\"\n",
        "    #YOU CAN CHANGE THE WHOLE AUGUMENTATION PROCESS\n",
        "\n",
        "    # Normalize the input features\n",
        "    x_train = (x_train - x_train.mean()) / x_train.std()\n",
        "    x_test = (x_test - x_test.mean()) / x_test.std()\n",
        "\n",
        "    # Function to add Gaussian noise\n",
        "    def add_gaussian_noise(X, mean=0, std=0.01):\n",
        "        noise = np.random.normal(mean, std, X.shape)\n",
        "        X_noisy = X + noise\n",
        "        return X_noisy\n",
        "\n",
        "    # Apply Gaussian noise to the training data\n",
        "    x_train_noisy = add_gaussian_noise(x_train)\n",
        "\n",
        "    # Combine original and noisy data\n",
        "    x_train_combined = np.vstack((x_train, x_train_noisy))\n",
        "    y_train_combined = np.vstack((y_train, y_train))\n",
        "\n",
        "    # Convert combined data back to non-categorical for SMOTE\n",
        "    y_train_combined_non_categorical = np.argmax(y_train_combined, axis=1)\n",
        "\n",
        "    # Apply SMOTE\n",
        "    smote = SMOTE()\n",
        "    x_train_final, y_train_final = smote.fit_resample(x_train_combined, y_train_combined_non_categorical)\n",
        "\n",
        "    print(x_train_final.shape, y_train_final.shape)\n",
        "\n",
        "    ''''''\n",
        "\n",
        "    # Convert y_train_final back to categorical\n",
        "    y_train_final = to_categorical(y_train_final, num_classes)\n",
        "\n",
        "    # YOU CAN CHANGE THE NUMBER OF NODES IN EACH LAYER, THE NUMBER OF LAYERS, THE ACTIVATION FUNCTIONS, THE REGULARIZATION,\n",
        "    # THE DROPOUT, THE BATCH NORMALIZATION, THE LEARNING RATE, THE OPTIMIZER, THE LOSS FUNCTION,  THE EPOCHS,\n",
        "    # THE BATCH SIZE, THE VALIDATION SPLIT, THE LEARNING RATE SCHEDULER, IDK WHAT ELSE THAT'S ALL I KNOW\n",
        "    # Define the model\n",
        "    model = Sequential([\n",
        "        Dense(128, input_shape=(x_train_final.shape[1],), activation='relu', kernel_regularizer=keras.regularizers.l2(0.001)),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.1),\n",
        "        Dense(64, activation='sigmoid', kernel_regularizer=keras.regularizers.l2(0.001)),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.1),\n",
        "        Dense(32, activation='sigmoid', kernel_regularizer=keras.regularizers.l2(0.001)),\n",
        "        BatchNormalization(),\n",
        "        Dense(16, activation='sigmoid', kernel_regularizer=keras.regularizers.l2(0.001)),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.1),\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.005),\n",
        "                loss='categorical_crossentropy',\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "    # Learning rate scheduler\n",
        "    def lr_schedule(epoch):\n",
        "        lr = 0.0001\n",
        "        if epoch > 150:\n",
        "            lr = 0.00005\n",
        "        elif epoch > 100:\n",
        "            lr = 0.0001\n",
        "        elif epoch > 50:\n",
        "            lr = 0.0002\n",
        "        print(f\"Learning rate: {lr}\")\n",
        "        return lr\n",
        "\n",
        "    lr_scheduler = keras.callbacks.LearningRateScheduler(lr_schedule)\n",
        "\n",
        "    # Train the model\n",
        "    history = model.fit(x_train_final, y_train_final, epochs=200, batch_size=2, validation_split=0.3, callbacks=[lr_scheduler])\n",
        "\n",
        "    # Evaluate the model\n",
        "    test_loss, test_accuracy = model.evaluate(x_test, y_test)\n",
        "\n",
        "    print(f'Test loss: {test_loss:.4f}')\n",
        "    print(f'Test accuracy: {test_accuracy:.4f}')\n",
        "\n",
        "    # Plotting training history\n",
        "    fig, axs = plt.subplots(2)\n",
        "\n",
        "    # Accuracy subplot\n",
        "    axs[0].plot(history.history[\"accuracy\"], label=\"train accuracy\")\n",
        "    axs[0].plot(history.history[\"val_accuracy\"], label=\"validation accuracy\")\n",
        "    axs[0].set_ylabel(\"Accuracy\")\n",
        "    axs[0].legend(loc=\"lower right\")\n",
        "    axs[0].set_title(\"Accuracy eval\")\n",
        "\n",
        "    # Loss subplot\n",
        "    axs[1].plot(history.history[\"loss\"], label=\"train loss\")\n",
        "    axs[1].plot(history.history[\"val_loss\"], label=\"validation loss\")\n",
        "    axs[1].set_ylabel(\"Loss\")\n",
        "    axs[1].set_xlabel(\"Epoch\")\n",
        "    axs[1].legend(loc=\"upper right\")\n",
        "    axs[1].set_title(\"Loss eval\")\n",
        "\n",
        "    plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}